{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "# Human-object interaction (HOI) problem domain Activity Detection üö∂‚Äç‚ôÇÔ∏èüö∂‚Äç‚ôÄÔ∏è\n",
        "\n",
        "Activity detection is very new as compared to more classical problems like object detection, this project work together with SIT-NVIDIA collaboration hopes to contribute to the advancement in this area. \n",
        "\n",
        "\n",
        "This notebook will allow us to easily switch between training/testing ML pipelines, models and other configuration settings using interactive UI. üíªüíª"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          0
        ],
        "init_cell": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Check your Python version is compatible with the notebook project.\n",
        "\n",
        "from platform import python_version\n",
        "\n",
        "# Lets make sure that your Juypter notebook is running Python 3.8.13.\n",
        "if(python_version() == '3.8.13'):\n",
        "    print(f\"Hello, welcome to the HOI Activity Detection Project! You are running 3.8.13 which is the correct version for this project!\")\n",
        "else:\n",
        "    print(f\"Oops! This is the wrong version! You are currently in {python_version()} but we need 3.8.13. Please change your kernel to Python 3.8.13 and try again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "# Imports and Setup\n",
        "Lets start with the base imports and installing dependencies. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          0
        ],
        "init_cell": true,
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Run this cell to install all the required dependencies needed for this notebook\n",
        "\n",
        "%pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# This notebook requires pytorch and a device with a CUDA 11.3 capabilities.\n",
        "%pip install timm==0.4.12 scikit-learn==1.1.1 numpy==1.23.0 tqdm==4.38.0 ipython==8.5.0 opencv-python==4.6.0.66  wandb==0.13.3  omegaconf==2.0.6 av==8.0.2 ipywidgets==8.0.2 pandas==1.5.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          0
        ],
        "init_cell": true,
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Run to import all dependencies\n",
        "import importlib \n",
        "import sys, inspect\n",
        "import datetime\n",
        "import os\n",
        "import shutil\n",
        "import requests\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import wandb\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from pathlib import Path\n",
        "from logging import raiseExceptions\n",
        "\n",
        "# For display.\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interactive\n",
        "from IPython.display import display, clear_output, YouTubeVideo\n",
        "from IPython.utils import io\n",
        "\n",
        "# For progress bar.\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from tqdm import tqdm\n",
        "# Run this to import all dependencies required for this notebook\n",
        "layout = widgets.Layout(width='auto') \n",
        "layout_hidden  = widgets.Layout(visibility = 'hidden')\n",
        "layout_visible = widgets.Layout(visibility = 'visible')\n",
        "\n",
        "cwd = Path.cwd()\n",
        "if \"tsu\" in str(cwd) or \"step\" in str(cwd) or \"MS-TCT\" in str(cwd):\n",
        "    %cd ..\n",
        "    cwd = Path.cwd()\n",
        "    \n",
        "tsu_wd = cwd /  \"tsu\"\n",
        "step_wd = cwd / \"step\"\n",
        "mstct_wd = cwd / \"MS-TCT\"\n",
        "\n",
        "def widget_slider(description, value,minimum, maximum):\n",
        "    slider = widgets.IntSlider(value=value,min=minimum,max=maximum,step=1,description=description,layout = layout, style={'description_width': 'initial'})\n",
        "    display(slider)\n",
        "    return(slider)\n",
        "\n",
        "def widget_textbox(description,value):\n",
        "    textbox = widgets.Text(placeholder = \"Enter the {0}\".format(description), description= description, value = value, layout = layout, style={'description_width': 'initial'})\n",
        "    display(textbox)\n",
        "    return(textbox)\n",
        "\n",
        "def widget_button(description):\n",
        "    button = widgets.Button(description = description, layout = layout, style={'description_width': 'initial'})\n",
        "    display(button)\n",
        "    return(button)\n",
        "\n",
        "def widget_dropdown(description, items):\n",
        "    dropdown = widgets.Dropdown(description=description,options=items,disabled=False)\n",
        "    display(dropdown)\n",
        "    return(dropdown)\n",
        "\n",
        "def widget_multiSelect(description,options,value=[]):\n",
        "    multiSelect = widgets.SelectMultiple(options=options,value=value,description=description,disabled=False, layout = layout, style={'description_width': 'initial'})\n",
        "    display(multiSelect)\n",
        "    return(multiSelect)\n",
        "\n",
        "def widget_output():\n",
        "    out = widgets.Output(layout={'border': '1px solid black','height':'auto','width':'auto','overflow':'auto'})\n",
        "    return(out)\n",
        "\n",
        "def widget_video(file_path,autoplay):\n",
        "    video = widgets.Video.from_file(file_path, controls=True, autoplay=autoplay, width=\"950\", height=\"600\")\n",
        "    display(video)\n",
        "    return(video)\n",
        "\n",
        "def widget_radio(description, items):\n",
        "    radio = widgets.RadioButtons(options=items, value=items[0],description=description,disabled=False)\n",
        "    display(radio)\n",
        "    return(radio)\n",
        "\n",
        "def widget_youtube_video(videoid,autoplay):\n",
        "    video = YouTubeVideo(videoid, autoplay=autoplay, width=\"950\", height=\"600\")\n",
        "    display(video)\n",
        "    return(video)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "# Pipeline Selection üë∑\n",
        "There are two algorithms available in this notebook.\n",
        "\n",
        "1. [Toyota Smart Home](https://project.inria.fr/toyotasmarthome). Toyota Smarthome Untrimmed (TSU) is targeting the activity detection task in long untrimmed videos. Therefore, in TSU, the entire recording when the person is visible. The dataset contains 536 videos with an average duration of 21 mins. The dataset is annotated with 51 activities. \n",
        "\n",
        "2. [Spatio-Temporal Progressive Learning for Video Action Detection](https://github.com/NVlabs/STEP) STEP a progressive learning framework for spatio-temporal action detection in videos. To learn more, the poster can be found at [Google Drive](https://drive.google.com/file/d/1GWWLH5HQM8FoEIutIOzvtURBI6y09NBr/view). For a more in-depth discussion, the paper can be read at [arxiv](https://arxiv.org/abs/1904.09288). It uses the [AVA Actions v2.1 dataset](https://research.google.com/ava/download.html). The dataset is annotated with 80 activities.\n",
        "\n",
        "3. [CVPR 2022 MS-TCT](https://github.com/dairui01/MS-TCT). Multi-Scale Temporal ConvTransformer for Action Detection\" on Charades dataset (Localization setting, i.e., Charades_v1_localize). To learn more, the paper can be found at [openmaccess](https://openaccess.thecvf.com/content/CVPR2022/papers/Dai_MS-TCT_Multi-Scale_Temporal_ConvTransformer_for_Action_Detection_CVPR_2022_paper.pdf)\n",
        "\n",
        "# Before You Begin\n",
        "\n",
        "## TSU - [Toyota Smart Home](https://project.inria.fr/toyotasmarthome)\n",
        "Please request for the untrimmed dataset available in the [Toyota Smart Home](https://project.inria.fr/toyotasmarthome) website.\n",
        "1. RGB - Videos in MP4 is required for feature extraction.\n",
        "    - Place it in `data/TSU/TSU_Videos_mp4/`\n",
        "\n",
        "## STEP - [Spatio-Temporal Progressive Learning for Video Action Detection](https://github.com/NVlabs/STEP)\n",
        "This algorithm is more involved, requires more effort to personally acquire the dataset as it is uploaded in youtube.\n",
        "\n",
        "1. Follow the [STEP README](https://github.com/NVlabs/STEP#installation) for installation instructions.\n",
        "    - Install APEX.\n",
        "    - Do not clone STEP as it is included in this repository.\n",
        "    - In a terminal, cd into STEP, and install external packages with `python setup.py build develop`\n",
        "2. Install ffmpeg - [Tutorial to add ffmpeg to path in Windows](https://www.youtube.com/watch?v=qjtmgCb8NcE&ab_channel=LinuxLeech)\n",
        "3. Download the dataset from youtube. In the `step/custom_utils` directory, there are scripts to download the videos.\n",
        "    1. Install yt-dlp : `pip install yt-dlp`\n",
        "    2. Run `python get_valid_youtube.py` to get the list of valid videos.\n",
        "    3. Run `python download_vids.py` to download the videos. (this will take quite a long while as the dataset is large)\n",
        "    4. There may be some copyright issues when downloading videos. \n",
        "        - In `get_valid_youtube.py` , comment and uncomment a specified block of code to remove videos that are not available from the train/val annotations.\n",
        "        - Run `python get_valid_youtube.py` to generate a new ava_train_v2.1_filter.csv and ava_val_v2.1_filter.csv.\n",
        "    5. Move videos into `step/datasets/ava/videos`\n",
        "    6. Generate labels using [Dataset Preparation](https://github.com/NVlabs/STEP#dataset-preparation)\n",
        "        - `python scripts/generate_label.py datasets/ava_val_v2.1_filter.csv`\n",
        "        - `python scripts/generate_label.py datasets/ava_train_v2.1_filter.csv`\n",
        "        - Move generated labels `val.pkl` and `train.pkl` into `datasets/ava/label`\n",
        "\n",
        "## MS-TCT [Multi-Scale Temporal ConvTransformer for Action Detection](https://github.com/dairui01/MS-TCT)\n",
        "MS-TCT is built on top of the pre-trained I3D features. Thus, feature extraction is needed before training the network.\n",
        "1. Please download the Charades dataset (24 fps version) from this [link](https://prior.allenai.org/projects/charades).\n",
        "2. Follow this [repository](https://github.com/piergiaj/pytorch-i3d) to extract the snippet-level I3D feature.\n",
        "\n",
        "\n",
        "\n",
        "After preparing the dataset and selecting a pipeline, In JupyterLab, __Kernel > Run Selected Cell and All Below.__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "pipelines = [\"TSU\",\"STEP\",\"MSTCT\"]\n",
        "pipeline_selected = \"TSU\"\n",
        "\n",
        "display_pipeline_output = widget_output()\n",
        "\n",
        "@display_pipeline_output.capture(clear_output=True,wait=True)\n",
        "def pipeline_onclick(args):\n",
        "    global pipeline_selected \n",
        "    pipeline_selected= pipeline_radio.value\n",
        "    if pipeline_selected == \"TSU\": \n",
        "        %cd {tsu_wd}\n",
        "        print(\"TSU selected. In Jupyter Lab, Kernel > Run Selected Cell and All Below\")\n",
        "    elif pipeline_selected == \"STEP\":\n",
        "        %cd {step_wd}\n",
        "        print(\"STEP selected. In Jupyter Lab, Kernel > Run Selected Cell and All Below\")\n",
        "    elif pipeline_selected == \"MSTCT\":\n",
        "        %cd {mstct_wd}\n",
        "        print(\"MS-TCT selected. In Jupyter Lab, Kernel > Run Selected Cell and All Below\")\n",
        "    else:\n",
        "        print(\"Please select a pipeline\")\n",
        "    print(\"This action sets the pipeline_selected variable to the selected pipeline and will be used as a reference for the rest of the ntoebook.\")\n",
        "\n",
        "pipeline_radio = widget_radio(\"Pipeline\", pipelines)\n",
        "pipeline_button = widget_button(\"Select Pipeline\")\n",
        "pipeline_button.on_click(pipeline_onclick)\n",
        "\n",
        "display(display_pipeline_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "# Before continuing, take note that some sections do not cover some pipelines.\n",
        "\n",
        "## TSU.\n",
        "### All sections available\n",
        "## STEP\n",
        "### Most section available except\n",
        "  - Feature Extraction\n",
        "  - Create Train/Test Split \n",
        "## MS-TCT\n",
        " - **Only Training and Evaluation available**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "# Data Exploration\n",
        "Let's explore some of the datasets available in this project.\n",
        "The following cells allows you to playback the selected video from the selected (TSU/STEP) project. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          0
        ],
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "if pipeline_selected == \"TSU\": \n",
        "    # TSU Data exploration\n",
        "    def get_data_dir():\n",
        "        data = os.listdir(data_path)\n",
        "        dataset_dirs = []\n",
        "\n",
        "        # Select only directories, not files.\n",
        "        for d in data:\n",
        "            if os.path.isdir(os.path.join(data_path, d)):\n",
        "                dataset_dirs.append(d)\n",
        "        return dataset_dirs\n",
        "\n",
        "    # Selected video path.\n",
        "    data_path = f\"{cwd}/data/\"\n",
        "    video_path = ''\n",
        "    display_dataset_output = widget_output()\n",
        "\n",
        "    @interact\n",
        "    def display_dataset(Dataset=get_data_dir()):\n",
        "        global video_path\n",
        "        global display_show_video\n",
        "        display_show_video = False\n",
        "        display_dataset_output.clear_output()\n",
        "        dataset_path = data_path + Dataset\n",
        "        video_path = ''\n",
        "        for d in os.listdir(dataset_path):\n",
        "            if os.path.isdir(os.path.join(data_path + Dataset, d)):\n",
        "                if 'video' in d.lower() and 'mp4' in d.lower():\n",
        "                    video_path = data_path + Dataset + '/' + d\n",
        "                    break\n",
        "\n",
        "        if video_path == '':\n",
        "            print('Oops, no video dataset found! Please try another dataset. To specify video directory, please have the keywords video and mp4 in the name of the directory.')\n",
        "        else:\n",
        "            choose_video = widgets.Dropdown(\n",
        "                options= os.listdir(video_path),\n",
        "                description='Video Name:',\n",
        "            )\n",
        "            @display_dataset_output.capture(clear_output=True,wait=True)\n",
        "            def handle_video_submit(sender):\n",
        "                with display_dataset_output:\n",
        "                        print(f\"Video Name: {choose_video.value}\")\n",
        "                        widget_video(f\"{video_path}/{choose_video.value}\",False)\n",
        "\n",
        "            choose_video_ui = interactive(handle_video_submit,sender = choose_video)\n",
        "            display(display_dataset_output)\n",
        "            display(choose_video_ui)\n",
        "        \n",
        "        \n",
        "\n",
        "if pipeline_selected == \"STEP\":\n",
        "    def load_ava():\n",
        "        df_val = pd.read_csv(f\"{step_wd}/datasets/ava_val_v2.1_filter.csv\", header=None, usecols=[0] , names=['videoid'])\n",
        "        df_train = pd.read_csv(f\"{step_wd}/datasets/ava_val_v2.1_filter.csv\", header=None, usecols=[0] , names=['videoid'])\n",
        "            \n",
        "        videoids = df_val[\"videoid\"].unique().tolist() + df_train[\"videoid\"].unique().tolist()\n",
        "        return videoids\n",
        "    display_step_dataset_output = widget_output()\n",
        "\n",
        "    \n",
        "    @display_step_dataset_output.capture(clear_output=True,wait=True)\n",
        "    def handle_step_submit(sender):\n",
        "        print(sender)\n",
        "        with display_step_dataset_output:\n",
        "            print(f\"Video Id: {choose_step_video_dropdown.value}\")\n",
        "            widget_youtube_video(choose_step_video_dropdown.value,True)\n",
        "\n",
        "\n",
        "    choose_step_video_dropdown = widgets.Dropdown(description=\"Video Id\",options=load_ava())\n",
        "    choose_step_video_ui = interactive(handle_step_submit,sender = choose_step_video_dropdown)\n",
        "    display(display_step_dataset_output)\n",
        "    display(choose_step_video_ui)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": []
      },
      "source": [
        "# Feature Extraction ‚úÇÔ∏è\n",
        "## Overview\n",
        "This section covers the feature extraction for selected video of your choice after data exploration. The feature extraction uses v-iashin video feature extraction method to obtain I3D RGB and Flow feature dataset. The feature set is later used for training of the model splitting it for train/test split.\n",
        "\n",
        "## Instructions\n",
        "1. Run the cell\n",
        "2. Select the dataset(file path with raw video data needs to be in root directory to be visible)\n",
        "3. Input the directory where you want the output feature data to be saved at\n",
        "4. Select the videos that you want to extract\n",
        "5. Review your settings and run the extraction button\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          0
        ],
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Feature extraction\n",
        "if pipeline_selected == \"TSU\": \n",
        "    feature_type = \"i3d\" #fixed for tsu pipeline\n",
        "    flow_type = \"raft\" #dont use pwc because it will generate flow \n",
        "    device = \"cuda:0\" #for multi-gpu usage, refer to github repo on v-iashin\n",
        "    on_extraction = \"save_numpy\" #there is option for save_numpy(.npy), save_pickle(.pk1)\n",
        "    extract_output_path = \"\"\n",
        "    streams = \"rgb\"\n",
        "    stack_size = 16\n",
        "    step_size = 16\n",
        "    extract_video_paths = \"\" #can be one video path or a list of video path e.g [\"file1.mp4\", \"file2.mp4\"]\n",
        "\n",
        "    # Utility to move files\n",
        "    def make_dir_and_move(target_dir,extension):\n",
        "        # make directory\n",
        "        save_dir_path = f\"{target_dir}/{extension}\"\n",
        "        Path(save_dir_path).mkdir(parents=True, exist_ok=True)\n",
        "        # move file with file extension\n",
        "        files = os.listdir(target_dir)\n",
        "        for file in files:\n",
        "            file_path = os.path.join(target_dir, file)\n",
        "            # if not directory\n",
        "            if not os.path.isdir(file_path) and extension in file:\n",
        "                new_file_name = file.split(\"_\",1)[0] + \".npy\"\n",
        "                old_file_path = f\"{Path().absolute()}/{target_dir}/{file}\"\n",
        "                new_file_path= f\"{Path().absolute()}/{target_dir}/{extension}/{new_file_name}\"\n",
        "                shutil.move(old_file_path,new_file_path)\n",
        "        print(f\"Features saved in: {Path().absolute().as_posix()}/{target_dir}/{extension}\")\n",
        "\n",
        "    # Run this to select the preferred dataset and videos you like to extract\n",
        "\n",
        "\n",
        "    # Run this to select the preferred dataset and videos you like to train.\n",
        "    def get_data_dir():\n",
        "        data = os.listdir(data_path)\n",
        "        dataset_dirs = []\n",
        "\n",
        "        # Select only directories, not files.\n",
        "        for d in data:\n",
        "            if os.path.isdir(os.path.join(data_path, d)):\n",
        "                dataset_dirs.append(d)\n",
        "        return dataset_dirs\n",
        "\n",
        "    # Selected video path.\n",
        "    data_path = f\"{cwd}/data/\"\n",
        "\n",
        "    # Selected video path.\n",
        "    extract_video_path = ''\n",
        "\n",
        "    extraction_output = widget_output()\n",
        "\n",
        "    @interact\n",
        "    def display_dataset(Dataset=get_data_dir()):\n",
        "        global extract_output_path\n",
        "        global extract_video_path\n",
        "        dataset_path = data_path + Dataset\n",
        "        extract_video_path = ''\n",
        "        for d in os.listdir(dataset_path):\n",
        "            if os.path.isdir(os.path.join(data_path + Dataset, d)):\n",
        "                if 'video' in d.lower() and 'mp4' in d.lower():\n",
        "                    extract_video_path = data_path + Dataset + '/' + d\n",
        "                    break\n",
        "                    \n",
        "        if extract_video_path == '':\n",
        "            print('Oops, no video dataset found! Please try another dataset. To specify video directory, please have the keywords video and mp4 in the name of the directory.')\n",
        "            \n",
        "        else:\n",
        "            global videos_in_dir\n",
        "            videos_in_dir = os.listdir(extract_video_path)\n",
        "            feature_extraction_output_textbox = widget_textbox(\"Feature output folder\",\"data/TSU/TSU_Video_features\")\n",
        "            print(\"Ctrl A to select all videos. Use Shift or Ctrl to select multiple\")\n",
        "            feature_extraction_input_videos = widget_multiSelect(\"Select videos to extract\",videos_in_dir)\n",
        "            @extraction_output.capture(clear_output=True)\n",
        "            \n",
        "            def feature_extraction_onclick(args):\n",
        "                selected_videos = list(feature_extraction_input_videos.value)\n",
        "                extract_output_root_path = feature_extraction_output_textbox.value\n",
        "                extract_output_path =  f\"{cwd.as_posix()}/{extract_output_root_path}\"\n",
        "                if len(selected_videos):\n",
        "                    selected_videos = [extract_video_path + \"/\"+ video  for video in selected_videos]\n",
        "                    selected_videos = str(selected_videos).replace(\"'\",\"\")\n",
        "                    input_videos = f\"\\\"{selected_videos}\\\"\"\n",
        "                    \n",
        "                    with extraction_output:\n",
        "                        print(\"Starting feature extraction\")\n",
        "                        \n",
        "                        if(Path().absolute().name != \"video_features\"): \n",
        "                            %cd {tsu_wd}/video_features\n",
        "                        !python main.py feature_type={feature_type} streams={streams} flow_type={flow_type} device={device} video_paths={input_videos} on_extraction={on_extraction} output_path={extract_output_path} stack_size={stack_size} step_size={step_size}\n",
        "                        %cd {cwd}\n",
        "                    # time to move files!\n",
        "                    print(extract_output_root_path)\n",
        "                    make_dir_and_move(f\"{extract_output_root_path}/i3d\", \"rgb\")\n",
        "                    # make_dir_and_move(f\"{extract_output_root_path}/i3d\", \"flow\")\n",
        "                    %cd {tsu_wd}\n",
        "                    \n",
        "                else:\n",
        "                    print(\"No videos selected.. please select at least one video before extracting features.\")\n",
        "\n",
        "            feature_extraction_button = widget_button(\"Extract features\")\n",
        "            feature_extraction_button.on_click(feature_extraction_onclick)\n",
        "            display(extraction_output)\n",
        "            \n",
        "else:\n",
        "    print(\"Only required for TSU. Move on!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Train/Test Split - TSU only üí™\n",
        "With the features extracted in the previous section, lets create a split csv.\n",
        "The feature directory can be found in the previous output, or the feature output folder you specified.\n",
        "\n",
        "1) Fill in the feature directory.\n",
        "2) Choose CS/CV split.\n",
        "    - CS: 34.5% testing, 65.5% training. \n",
        "    - CV: 31% testing, 69% training.\n",
        "    - Based on 536 video features available. Results will vary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "split_generate_output = widget_output()\n",
        "if pipeline_selected == \"TSU\": \n",
        "    @split_generate_output.capture(clear_output=True)\n",
        "    def split_generate_onclick(args):\n",
        "        feature_dir = feature_directory_textbox.value\n",
        "        output_path = split_output_json_textbox.value\n",
        "        selected_split = split_setting_dropdown.value\n",
        "        split_setting = f\"{cwd.as_posix()}/data/TSU/{'smarthome_CS_51.json' if selected_split == 'CS' else 'smarthome_CV_51.json'}\"\n",
        "        %run -m validate_train_test -feature_dir {feature_dir} -output_path {output_path} -split_setting {split_setting}\n",
        "\n",
        "    feature_directory_textbox = widget_textbox(\"Feature directory\",f\"{cwd.as_posix()}/data/TSU/TSU_Video_features/i3d/rgb\")\n",
        "    split_output_json_textbox = widget_textbox(\"Split Output\",f\"{cwd.as_posix()}/data/TSU/my_new_split.json\")\n",
        "    split_setting = [\"CS\",\"CV\"] \n",
        "    split_setting_dropdown = widget_dropdown(\"Split_setting\", split_setting)\n",
        "    split_generate_button = widget_button(\"Generate split\")\n",
        "    split_generate_button.on_click(split_generate_onclick)\n",
        "\n",
        "    display(split_generate_output)\n",
        "else:\n",
        "    print(\"Only required for TSU. Move on!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference üîç\n",
        "Result will return you with a video with captions in each frame indicating the current action.\n",
        "## Overview\n",
        "By running the inference cell below, it will return a video with captions in each frame indicating what the machine learning model thinks the subject is doing.\n",
        "\n",
        "### For STEP,\n",
        "Only TSU video files or videosets that are available locally can be used. Place it inside ./data/DATASET/DATASET_Videos_mp4/123456.mp4\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1) Select a dataset folder from the dropdown list\n",
        "\n",
        "2) Select a video from the dropdown list\n",
        "\n",
        "3) ONLY FOR TSU. Indicate whether you want to watch the video playback in real-time using the True/False radio buttons\n",
        "- If you indicated True, a video popup will appear\n",
        "- If you indicated False, you can only view the inference video manually in the directory after the inference process has completed\n",
        "\n",
        "5) Press the \"Start Inference\" button\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          0
        ],
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Select the model, dataset and video for inference\n",
        "    # output for video\n",
        "display_inference_output = widget_output()\n",
        "display_inference_result_output = widget_output()\n",
        "\n",
        "if pipeline_selected == \"TSU\": \n",
        "\n",
        "    model_path = \"./model/trained\" \n",
        "\n",
        "    # Selected video path.\n",
        "    data_path = f\"{cwd.as_posix()}/data/\"\n",
        "    # UI Variables to pass into python program\n",
        "    # Add fake dataset..?\n",
        "    inference_dataset = \"TSU\"\n",
        "    # Model currently only has PDAN. \n",
        "    model = \"PDAN\"\n",
        "    # AP type dont need to change. No UI required.\n",
        "    APtype =  \"map\"\n",
        "    # Batch Size ... No UI required.\n",
        "    batch_size = \"1\"\n",
        "    # should display model in ./model/trained\n",
        "    load_model = './PDAN_TSU_RGB'\n",
        "    # the rgb/flow features\n",
        "    root = f'{cwd.as_posix()}/data/TSU/TSU_RGB_i3d_feat/RGB_i3d_16frames_64000_SSD' \n",
        "    # Add models to list\n",
        "    model_list = [os.path.join(dp, f) for dp, dn, filenames in os.walk(model_path) for f in filenames if os.path.splitext(f)[1] != '.pyc' and os.path.splitext(f)[1] != '.py']\n",
        "    model_list = [each.replace(\"\\\\\", \"/\") for each in model_list]\n",
        "\n",
        "    # Some issues with pretrained model, has to be inside src.\n",
        "    model_list.insert(0,\"./PDAN_TSU_RGB\")\n",
        "\n",
        "    def get_data_dir():\n",
        "        data = os.listdir(data_path)\n",
        "        dataset_dirs = []\n",
        "\n",
        "        # Select only directories, not files.\n",
        "        for d in data:\n",
        "            if os.path.isdir(os.path.join(data_path, d)):\n",
        "                dataset_dirs.append(d)\n",
        "        return dataset_dirs\n",
        "\n",
        "\n",
        "    # Global Video Path Variable\n",
        "    inference_video_path = ''\n",
        "    inference_dataset_videos = []\n",
        "\n",
        "\n",
        "\n",
        "    @interact\n",
        "    def display_models(Model=model_list):\n",
        "        global load_model\n",
        "        load_model = Model\n",
        "\n",
        "\n",
        "    @interact\n",
        "    def display_dataset(Dataset=get_data_dir()):\n",
        "        dataset_path = data_path + Dataset\n",
        "        global inference_dataset\n",
        "        inference_dataset = Dataset\n",
        "        global choose_inference_video\n",
        "        global inference_video_path\n",
        "        global inference_dataset_videos\n",
        "        global display_inference_output\n",
        "        global display_inference_result_output\n",
        "        inference_video_path = ''\n",
        "        for d in os.listdir(dataset_path):\n",
        "            if os.path.isdir(os.path.join(data_path + Dataset, d)):\n",
        "                if 'video' in d.lower() and 'mp4' in d.lower():\n",
        "                    inference_video_path = data_path + Dataset + '/' + d\n",
        "                    break\n",
        "        # clear when selecting new dataset\n",
        "        display_inference_output.clear_output()\n",
        "        if inference_video_path == '':\n",
        "            inference_dataset_videos = []\n",
        "            print('Oops, no video dataset found! Please try another dataset. To specify video directory, please have the keywords video and mp4 in the name of the directory.')\n",
        "        else:\n",
        "            inference_dataset_videos = [Path(video).with_suffix('').name for video in os.listdir(inference_video_path)]\n",
        "\n",
        "            choose_inference_video = widgets.Dropdown(\n",
        "                options= inference_dataset_videos,\n",
        "                description='Video Name:',\n",
        "            )\n",
        "\n",
        "            @display_inference_output.capture(clear_output=True)\n",
        "            def handle_inference_video_submit(sender):\n",
        "                with display_inference_output:\n",
        "                    print(f\"Video Name: {choose_inference_video.value}\")\n",
        "                    widget_video(f\"{inference_video_path}/{choose_inference_video.value}.mp4\", False)\n",
        "\n",
        "\n",
        "            display(display_inference_output)\n",
        "            choose_video_ui = interactive(handle_inference_video_submit,sender = choose_inference_video)\n",
        "            display(choose_video_ui)\n",
        "\n",
        "            popup_video_radio = widgets.RadioButtons(\n",
        "                options=['True', 'False'],\n",
        "                description='Playback',\n",
        "                disabled=False\n",
        "            )\n",
        "            display(popup_video_radio)\n",
        "            @display_inference_result_output.capture(clear_output=True)\n",
        "            def start_inference_onclick(args):\n",
        "                print(choose_inference_video.value)\n",
        "                with display_inference_result_output:\n",
        "                    %run inference.py -dataset {inference_dataset} -model {model} -APtype {APtype} -batch_size {batch_size} -load_model {load_model} -root {root} -video_name {choose_inference_video.value} \n",
        "                    print(f\"Creating inference video {choose_inference_video.value} with pop up: {popup_video_radio.value}\")\n",
        "                    %run -m create_video -selected_video {choose_inference_video.value} -pop_up {popup_video_radio.value}\n",
        "                    video_path = \"./inference_video\"\n",
        "                    widget_video(f\"{video_path}/inference_{choose_inference_video.value}.mp4\", True)\n",
        "            start_inference_button = widget_button(\"Start Inference\")\n",
        "            start_inference_button.on_click(start_inference_onclick)\n",
        "            display(display_inference_result_output)\n",
        "            \n",
        "if pipeline_selected == \"STEP\": \n",
        "    def get_data_dir():\n",
        "        data = os.listdir(data_path)\n",
        "        dataset_dirs = []\n",
        "\n",
        "        # Select only directories, not files.\n",
        "        for d in data:\n",
        "            if os.path.isdir(os.path.join(data_path, d)):\n",
        "                dataset_dirs.append(d)\n",
        "        return dataset_dirs\n",
        "    \n",
        "    \n",
        "    def set_frame_save_location(video_id):\n",
        "        ## saving file\n",
        "        save_dir_path = f\"./datasets/demo/frames/{video_id}\"\n",
        "        Path(save_dir_path).mkdir(parents=True, exist_ok=True)\n",
        "        return save_dir_path\n",
        "\n",
        "    def extract_frame(video_path):\n",
        "        clip_length = 1  # seconds\n",
        "        clip_time_padding = 1.0  # seconds\n",
        "        video_id = os.path.basename(video_path)\n",
        "        clip_dir = set_frame_save_location(Path(video_id).stem)\n",
        "        print(\"Working on\", video_path)\n",
        "        ffmpeg_command = f\"ffmpeg -i {video_path} -start_number 0 -qscale:v 4 {os.path.join(clip_dir,'%06d.jpg')}\"\n",
        "        subprocess.call(ffmpeg_command, shell=True)\n",
        "        \n",
        "        \n",
        "    # Selected video path.\n",
        "    data_path = f\"{cwd}/data/\"\n",
        "    @interact\n",
        "    def display_dataset(Dataset=get_data_dir()):\n",
        "        dataset_path = data_path + Dataset\n",
        "        global choose_step_inference_video\n",
        "        global step_inference_video_path\n",
        "        global step_inference_dataset_videos\n",
        "        for d in os.listdir(dataset_path):\n",
        "            if os.path.isdir(os.path.join(dataset_path, d)):\n",
        "                if 'video' in d.lower() and 'mp4' in d.lower():\n",
        "                    step_inference_video_path = data_path + Dataset + '/' + d\n",
        "                    break\n",
        "        print(step_inference_video_path)\n",
        "        if step_inference_video_path == '':\n",
        "            step_inference_dataset_videos = []\n",
        "            print('Oops, no video dataset found! Please try another dataset. To specify video directory, please have the keywords video and mp4 in the name of the directory.')\n",
        "        else:\n",
        "            step_inference_dataset_videos = [Path(video).with_suffix('').name for video in os.listdir(step_inference_video_path)]\n",
        "            \n",
        "            choose_inference_video = widgets.Dropdown(\n",
        "                options= step_inference_dataset_videos,\n",
        "                description='Video Name:',\n",
        "            )\n",
        "\n",
        "            #to display the output\n",
        "            @display_inference_output.capture(clear_output=True)\n",
        "            def handle_inference_video_submit(sender):\n",
        "                with display_inference_output:\n",
        "                    print(f\"Video Name: {choose_inference_video.value}\")\n",
        "                    widget_video(f\"{step_inference_video_path}/{choose_inference_video.value}.mp4\", False)\n",
        "            display(display_inference_output)\n",
        "            choose_video_ui = interactive(handle_inference_video_submit,sender = choose_inference_video)\n",
        "            display(choose_video_ui)\n",
        "            @display_inference_result_output.capture(clear_output=True)\n",
        "            def start_inference_onclick(args):\n",
        "                chosen_video_path = f\"{step_inference_video_path}/{choose_inference_video.value}.mp4\"\n",
        "                demo_frame_path = f\"{step_wd.as_posix()}/datasets/demo/frames/\"\n",
        "                with display_inference_result_output:\n",
        "                    print(f\"Extracting frames from {choose_inference_video.value}\")\n",
        "                    extract_frame(chosen_video_path)\n",
        "                    print(f\"Inferencing frames from {demo_frame_path}...\")\n",
        "                    %run demo.py -data_root {demo_frame_path}\n",
        "                    print(f\"Creating video from inference results {choose_inference_video.value}...\")\n",
        "                    %run -m create_video -selected_video {choose_inference_video.value} -path {step_wd.as_posix()}\n",
        "                    video_path = \"./inference_video\"\n",
        "                    widget_video(f\"{video_path}/inference_{choose_inference_video.value}.mp4\", True)\n",
        "            start_inference_button = widget_button(\"Start Inference\")\n",
        "            start_inference_button.on_click(start_inference_onclick)\n",
        "            display(display_inference_result_output)\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "source": [
        "# Training Section üöû\n",
        "\n",
        "## Overview\n",
        "\n",
        "Training is a process where a machine learning model learns from the associated training data.\n",
        "Models are found in model folder. Select the Architecture and the model config from the two drop down.\n",
        "\n",
        "## TSU Pipeline\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1) Set batch_size using the slider\n",
        "- batch_size refers to the number of training samples to work through before the update of internal model parameters\n",
        "- By decreasing the batch_size, the process gets faster at the expense of the performance\n",
        "\n",
        "2) Set Epoch using the slider\n",
        "- Epoch refers to the number of times the algorithm will work through the entire training\n",
        "- The number of epochs is the number of complete passes in the training dataset\n",
        "\n",
        "3) Enter desired name for the trained model through the input field\n",
        "\n",
        "4) Press the \"Train Model!\" button\n",
        "\n",
        "## STEP Pipeline\n",
        "\n",
        "1) Set batch_size using the slider\n",
        "- batch_size refers to the number of training samples to work through before the update of internal model parameters\n",
        "- By decreasing the batch_size, the process gets faster at the expense of the performance\n",
        "\n",
        "2) Set Epoch using the slider\n",
        "- Epoch refers to the number of times the algorithm will work through the entire training\n",
        "- The number of epochs is the number of complete passes in the training dataset\n",
        "\n",
        "3) Enter desired name for the trained model through the input field\n",
        "\n",
        "4) Press the \"Train Model!\" button\n",
        "\n",
        "Note: STEP has other parameters that can be explored in the code itself. For simplicity, only the selected few are provided\n",
        "\n",
        "## MS-TCT Pipeline\n",
        "\n",
        "1) Set batch_size using the slider\n",
        "- batch_size refers to the number of training samples to work through before the update of internal model parameters\n",
        "- By decreasing the batch_size, the process gets faster at the expense of the performance\n",
        "\n",
        "2) Set Epoch using the slider\n",
        "- Epoch refers to the number of times the algorithm will work through the entire training\n",
        "- The number of epochs is the number of complete passes in the training dataset\n",
        "\n",
        "Note: MS-TCT has other parameters that can be explored in the code itself. For simplicity, only the selected few are provided\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          0
        ],
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "\n",
        "training_output = widget_output()\n",
        "if pipeline_selected == \"TSU\": \n",
        "    @training_output.capture(clear_output=True, wait=True)\n",
        "    def on_train(arg):\n",
        "        split_setting= split_json_textbox.value\n",
        "        batch_size = train_batch_size_slider.value\n",
        "        epoch = train_epoch_slider.value\n",
        "        trained_model_name = trained_model_name_textbox.value\n",
        "        trained_root = trained_root_textbox.value\n",
        "        lr = learning_rate_textbox.value\n",
        "        kernel_size = kernel_size_textbox.value\n",
        "        num_channel = num_channel_textbox.value\n",
        "        ap_type = ap_type_radio.value\n",
        "        with training_output:\n",
        "            print(\"Training in progress...\")\n",
        "            %run train.py -dataset {dataset} -split_setting {split_setting} -model {model} -num_channel {num_channel} -lr {lr} -kernelsize {kernel_size} -APtype {ap_type} -epoch {epoch} -batch_size {batch_size} -comp_info {comp_info} -load_model {load_model} -root {root} -trained_model_name {trained_model_name}\n",
        "\n",
        "    # variables\n",
        "    dataset = \"TSU\"\n",
        "    model = \"PDAN\"\n",
        "    num_channel = 512\n",
        "    lr = 0.0002\n",
        "    kernel_size = 3\n",
        "    ap_type = \"map\"\n",
        "    epoch = 10\n",
        "    batch_size = 1\n",
        "    comp_info = \"TSU_CS_RGB_PDAN\"\n",
        "    load_model = 'False'\n",
        "    root = f'{cwd.as_posix()}/data/TSU/TSU_RGB_i3d_feat/RGB_i3d_16frames_64000_SSD'\n",
        "    trained_model_name = \"best_model_ever\" \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    print(\"How many samples per batch to load (default: 2)\")\n",
        "    train_batch_size_slider = widget_slider(\"Batch size\", 2,1,4)\n",
        "    print(\"Total number of iterations of all the training data in one cycle for training\")\n",
        "    train_epoch_slider = widget_slider(\"Epoch\", 1000, 1, 1000 )\n",
        "    print(\"Where is the split json data at?\")\n",
        "    split_json_textbox = widget_textbox(\"Split json\",f\"{cwd.as_posix()}/data/TSU/smarthome_CS_51.json\")\n",
        "    print(\"Where is the trained root at?\")\n",
        "    trained_root_textbox = widget_textbox('Feature directory',f'{cwd.as_posix()}/data/TSU/TSU_RGB_i3d_feat/RGB_i3d_16frames_64000_SSD')\n",
        "    print(\"Select the model to train with\")\n",
        "    trained_model_name_textbox = widget_textbox('New trained model name', \"the_best_model_ever\")\n",
        "    print(\"Tuning parameter in an optimization algorithm that determines the step size at each iteration\")\n",
        "    learning_rate_textbox = widget_textbox(\"Learning rate\", \"0.0002\")\n",
        "    print(\"What is the kernel size?\")\n",
        "    kernel_size_textbox = widget_textbox(\"Kernel Size\", \"3\")\n",
        "    print(\"How many channels is there?\")\n",
        "    num_channel_textbox = widget_textbox(\"Number of Channels\", \"512\")\n",
        "    print(\"Select your AP Type, either map or wap\")\n",
        "    ap_type_radio = widget_radio(\"AP Type\", [\"map\",\"wap\"])\n",
        "    train_button = widgets.Button(description = 'Train Model')   \n",
        "    train_button.on_click(on_train)\n",
        "    \n",
        "\n",
        "\n",
        "    display(train_button)\n",
        "    display(training_output)\n",
        "    \n",
        "    \n",
        "\n",
        "if pipeline_selected == \"MSTCT\": \n",
        "    @training_output.capture(clear_output=True, wait=True)\n",
        "    def on_train(arg):\n",
        "        batch_size = train_batch_size_slider.value\n",
        "        epoch = train_epoch_slider.value\n",
        "        with training_output:\n",
        "            print(\"Training in progress...\")\n",
        "            %run train.py -dataset {dataset} -mode {mode} -model {model} -train {train} -num_clips {num_clips} -skip {skip} -lr {lr} -comp_info {comp_info} -epoch {epoch} -unisize {unisize} -alpha_l {alpha_l} -beta_l {beta_l} -batch_size {batch_size}\n",
        "\n",
        "    # variables\n",
        "    dataset = \"charades\"\n",
        "    mode = \"rgb\"\n",
        "    model = \"MS_TCT\"\n",
        "    train = \"True\"\n",
        "    num_clips = \"256\"\n",
        "    skip = \"0\"\n",
        "    lr = 0.0001\n",
        "    comp_info = \"False\"\n",
        "    epoch = 50\n",
        "    unisize = \"True\"\n",
        "    alpha_l = \"1\"\n",
        "    beta_l = \"0.05\"\n",
        "    batch_size = 32\n",
        "\n",
        "    train_batch_size_slider = widget_slider(\"Batch size\", 2,1,4)\n",
        "    train_epoch_slider = widget_slider(\"Epoch\", 1000, 1, 1000 )\n",
        "    train_button = widgets.Button(description = 'Train Model')   \n",
        "    train_button.on_click(on_train)\n",
        "\n",
        "\n",
        "    display(train_button)\n",
        "    display(training_output)\n",
        "    \n",
        "if pipeline_selected == \"STEP\": \n",
        "    @training_output.capture(clear_output=True, wait=True)\n",
        "    def on_train(arg):\n",
        "        batch_size = train_batch_size_slider.value\n",
        "        max_epochs = train_epoch_slider.value\n",
        "        save_root = trained_root_textbox.value\n",
        "        with training_output:\n",
        "            print(\"Training in progress...\")\n",
        "            # run the script\n",
        "            %run -m train -- --data_root {data_root} --save_root {save_root} \\\n",
        "            --name {name} --pretrain_path {pretrain_path} --resume_path {resume_path} \\\n",
        "            --base_net {base_net} --det_net {det_net} --max_iter {max_iter} --T {T} \\\n",
        "            --iterative_mode {iterative_mode} --anchor_mode {anchor_mode} --anchor_mode {anchor_mode} --temporal_mode {temporal_mode} \\\n",
        "            --pool_mode {pool_mode} --pool_size {pool_size} --save_step {save_step} --topk {topk} --evaluate_topk {evaluate_topk} \\\n",
        "            --num_workers {num_workers} --max_epochs {max_epochs} --batch_size {batch_size} --print_step {print_step} \\\n",
        "            --optimizer {optimizer} --base_lr {base_lr} --det_lr {det_lr} --det_lr0 {det_lr0} --milestones {milestones} \\\n",
        "            --scale_norm {scale_norm} --do_flip {do_flip} --do_crop {do_crop} --do_photometric {do_photometric} --do_erase {do_erase} \\\n",
        "            --fc_dim {fc_dim} --dropout {dropout} --NUM_SAMPLE {NUM_SAMPLE} --scheduler {scheduler} --warmup_iters {warmup_iters} \\\n",
        "            --cls_thresh {cls_thresh} --reg_thresh {reg_thresh} --max_pos_num {max_pos_num} --neg_ratio {neg_ratio} \\\n",
        "            --freeze_affine {freeze_affine} --freeze_stats {freeze_stats} --lambda_reg {lambda_reg} --lambda_neighbor {lambda_neighbor} \n",
        "         \n",
        "    data_root = f\"{step_wd.as_posix()}/datasets/ava/\"\n",
        "    save_root = f\"{step_wd.as_posix()}/datasets/ava/cache/\"\n",
        "    pretrain_path = \"pretrained/ava_cls.pth\"\n",
        "\n",
        "    name = \"STEP\"\n",
        "    base_net = \"i3d\"\n",
        "    det_net = \"two_branch\"\n",
        "    resume_path = \"Auto\"\n",
        "\n",
        "    T = 3\n",
        "    max_iter = 3    # index starts from 1\n",
        "    iterative_mode = \"temporal\"\n",
        "    anchor_mode = \"1\"\n",
        "    temporal_mode = \"predict\"\n",
        "    pool_mode = \"align\"\n",
        "    pool_size = 7\n",
        "\n",
        "    # training schedule\n",
        "    num_workers = 16\n",
        "    max_epochs = 14\n",
        "    batch_size = 8\n",
        "    optimizer = \"adam\"\n",
        "    base_lr = 7.5e-5\n",
        "    det_lr0 = 1.5e-4\n",
        "    det_lr = 7.5e-4\n",
        "    save_step = 11465\n",
        "    print_step = 500\n",
        "    scheduler = \"cosine\"\n",
        "    milestones = \"-1\"\n",
        "    warmup_iters = 1000\n",
        "\n",
        "    # losses\n",
        "    dropout = 0.3\n",
        "    fc_dim = 256\n",
        "    lambda_reg = 5\n",
        "    lambda_neighbor = 1\n",
        "    cls_thresh = \"0.2,0.35,0.5\"\n",
        "    reg_thresh = \"0.2,0.35,0.5\"\n",
        "    max_pos_num = 5\n",
        "    neg_ratio = 2\n",
        "    NUM_SAMPLE = -1\n",
        "    topk = 300\n",
        "    evaluate_topk = 300\n",
        "\n",
        "    # data augmentation / normalization\n",
        "    scale_norm=2    # for i3d\n",
        "    do_flip=\"True\"\n",
        "    do_crop=\"True\"\n",
        "    do_photometric=\"True\"\n",
        "    do_erase=\"True\"\n",
        "    freeze_affine=\"True\"\n",
        "    freeze_stats=\"True\"\n",
        "\n",
        "    train_batch_size_slider = widget_slider(\"batch_size\", 8,1,32)\n",
        "    train_epoch_slider = widget_slider(\"Epoch\", 14, 1, 1000 )\n",
        "    trained_root_textbox = widget_textbox('Enter your save root',f'{step_wd.as_posix()}/datasets/ava/cache/')\n",
        "    train_button = widgets.Button(description = 'Train Model')   \n",
        "    train_button.on_click(on_train)\n",
        "\n",
        "\n",
        "    display(train_button)\n",
        "    display(training_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "#link your wandb\n",
        "wandb.login()\n",
        "# Display your project workspace\n",
        "%wandb ict3104-team14-2022/nvda-ml-activity-detection -h 2048\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "# Test üìù\n",
        "## Overview\n",
        "The evaluation process is used to analyze a model's performance by testing it against the ground truth.\n",
        "## TSU pipeline\n",
        "### Instructions\n",
        "\n",
        "1) Set batch_size using the slider\n",
        "- batch_size refers to the number of training samples to work through before the update of internal model parameters\n",
        "- By decreasing the batch_size, the process gets faster at the expense of the performance\n",
        "\n",
        "2) Select model from the the drop down\n",
        "\n",
        "3) Press the \"Evaluate Model\" button\n",
        "\n",
        "## How to read the result of the evaluation?\n",
        "\n",
        "After the evaluation process completes, it will return the following details about each video sample\n",
        "\n",
        "1) Mean Average Precision (mAP)\n",
        "- This value is used to analyze the accuracy of the object detection model.\n",
        "- The higher the mAP value, the more accurate the model.\n",
        "\n",
        "2) Average Precision Per Activity Class\n",
        "- This is also used to measure the accuracy of the model but it is responsible for individual classes.\n",
        "- There are 51 classes in total and an example is the make_coffee class.\n",
        "- For classes with the value 0, it suggests that the class is not present in the video sample.\n",
        "\n",
        "3) Shape\n",
        "- The shape variable is an array consisting of 2 values.\n",
        "- The first value of \"51\" refers to the number of classes which also refers to the action that the subject is performing.\n",
        "- The second value refers to the length of the video sample divided by 16.\n",
        "\n",
        "## STEP pipeline\n",
        "### Instructions\n",
        "\n",
        "The STEP test script works in a slightly different manner.\n",
        "It first loads a pretrained model as well as the configuration from your pretrained model. To test, rename your pretrained model to 'ava_step.pth' and place it into the directory as 'step/pretrained/ava_step.pth'.\n",
        "\n",
        "## MS-TCT pipeline\n",
        "### Instructions\n",
        "Takes in a pretrained model. Set the Pickle file Path to be tested.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          0
        ],
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# To evaluate the trained model chosen based on preferred configuration\n",
        "\n",
        "evaluation_output = widget_output()\n",
        "\n",
        "# --------------------------- TSU Pipeline ----------------------------------\n",
        "\n",
        "if pipeline_selected == \"TSU\": \n",
        "    @evaluation_output.capture(clear_output=True, wait=True)\n",
        "    def evaluate_onclick(args):\n",
        "        split_setting= split_json_textbox.value\n",
        "        batch_size = eval_batch_size_slider.value\n",
        "        load_model = load_model_textbox.value\n",
        "        root = evaluate_root_textbox.value\n",
        "        ap_type = ap_type_radio.value\n",
        "\n",
        "        with evaluation_output:\n",
        "            print(\"Starting evaluation\")\n",
        "            %run evaluation.py -dataset {dataset} -split_setting {split_setting} -model {model} -APtype {AP_type} -batch_size {batch_size} -load_model {load_model} -root {root}\n",
        "            print(\"Done!\")\n",
        "\n",
        "\n",
        "    dataset = \"TSU\"\n",
        "    model = \"PDAN\"\n",
        "    AP_type =  \"map\"\n",
        "    load_model = './PDAN_TSU_RGB'\n",
        "\n",
        "    model_path = \"./model/trained\" \n",
        "    # Add models to list\n",
        "    model_list = [os.path.join(dp, f) for dp, dn, filenames in os.walk(model_path) for f in filenames if os.path.splitext(f)[1] != '.pyc' and os.path.splitext(f)[1] != '.py']\n",
        "    model_list = [each.replace(\"\\\\\", \"/\") for each in model_list]\n",
        "    model_list.insert(0,\"./PDAN_TSU_RGB\")\n",
        "\n",
        "\n",
        "    # feature file\n",
        "    time_now  = datetime.datetime.now().strftime('%m_%d_%Y_%H_%M_%S') \n",
        "    result_name = f\"./results/{load_model}_{time_now}.txt\"\n",
        "    load_model_textbox = widget_dropdown(\"load_model\", model_list)\n",
        "    eval_batch_size_slider = widget_slider(\"batch_size\", 2,1,4)\n",
        "    split_json_textbox = widget_textbox(\"Split json\",f\"{cwd.as_posix()}/data/TSU/smarthome_CS_51.json\")\n",
        "    evaluate_root_textbox = widget_textbox('Feature directory',f'{cwd.as_posix()}/data/TSU/TSU_RGB_i3d_feat/RGB_i3d_16frames_64000_SSD')\n",
        "    ap_type_radio = widget_radio(\"AP Type\", [\"map\",\"wap\"])\n",
        "    evaluate_button = widget_button(\"Test TSU Model\")\n",
        "\n",
        "    evaluate_button.on_click(evaluate_onclick)\n",
        "\n",
        "    display(evaluation_output)\n",
        "    \n",
        "# --------------------------- MSTCT Pipeline ----------------------------------\n",
        "\n",
        "if pipeline_selected == \"MSTCT\":\n",
        "    \n",
        "    @evaluation_output.capture(clear_output=True, wait=True)\n",
        "    def evaluate_onclick(args):\n",
        "        pkl_path =  pickle_path_textbox.value\n",
        "        with evaluation_output:\n",
        "            print(\"Starting evaluation\")\n",
        "            %run -m Evaluation -pkl_path {pkl_path}\n",
        "            print(\"Done!\")\n",
        "    \n",
        "    pickle_path_textbox = widget_textbox(\"Pickle Path\",f\"{mstct_wd.as_posix()}/save_logit/example_epoch.pkl\")\n",
        "    evaluate_button = widget_button(\"Test MS_TCT Model\") \n",
        "    evaluate_button.on_click(evaluate_onclick)\n",
        "    display(evaluation_output)\n",
        "\n",
        "# --------------------------- STEP Pipeline ----------------------------------\n",
        "\n",
        "\n",
        "if pipeline_selected == \"STEP\": \n",
        "    @evaluation_output.capture(clear_output=True, wait=True)\n",
        "    def evaluate_onclick(args):\n",
        "        with evaluation_output:\n",
        "            print(\"Starting evaluation\")\n",
        "            %run -m eval\n",
        "            print(\"Done!\")\n",
        "    evaluate_button = widget_button(\"Test STEP Model\")\n",
        "\n",
        "    evaluate_button.on_click(evaluate_onclick)\n",
        "\n",
        "    display(evaluation_output)"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Initialization Cell",
    "kernelspec": {
      "display_name": "Python (Activity Detection)",
      "language": "python",
      "name": "ipykernel-activity-detection"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {},
        "version_major": 2,
        "version_minor": 0
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
